{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Data Columns: Index(['id', 'original_id', 'source', 'title', 'author', 'publication_date',\n",
      "       'content', 'chunk_id', 'cleaned_content'],\n",
      "      dtype='object')\n",
      "Public Data Columns: Index(['id', 'text', 'author', 'created_utc', 'permalink', 'score',\n",
      "       'cleaned_text'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vesper/Desktop/LSE/Capstone Project/dissertation/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for expert data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55a9bbd01864c66973449d60185286f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for public data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320355265db34242a68ab63e4f5561d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving expert embeddings to the database...\n",
      "Saving public embeddings to the database...\n",
      "Embeddings calculated and saved to the SQLite database.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = \"/Users/vesper/Desktop/LSE/Capstone Project/dissertation/arctic_shift/filtered_data/relevant_data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load the cleaned expert and public datasets\n",
    "expert_data = pd.read_sql_query(\"SELECT * FROM cleaned_expert_data\", conn)\n",
    "public_data = pd.read_sql_query(\"SELECT * FROM cleaned_public_data\", conn)\n",
    "\n",
    "# Ensure the correct columns are accessed\n",
    "print(\"Expert Data Columns:\", expert_data.columns)\n",
    "print(\"Public Data Columns:\", public_data.columns)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Calculate embeddings for expert and public data\n",
    "print(\"Calculating embeddings for expert data...\")\n",
    "expert_embeddings = embedding_model.encode(expert_data['cleaned_content'].tolist(), show_progress_bar=True)\n",
    "print(\"Calculating embeddings for public data...\")\n",
    "public_embeddings = embedding_model.encode(public_data['cleaned_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Save embeddings to the database\n",
    "expert_embeddings_df = pd.DataFrame(expert_embeddings)\n",
    "public_embeddings_df = pd.DataFrame(public_embeddings)\n",
    "\n",
    "# Adding 'id' column to match the original data for merging later\n",
    "expert_embeddings_df['id'] = expert_data['id']\n",
    "public_embeddings_df['id'] = public_data['id']\n",
    "\n",
    "# Save to new tables in the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "print(\"Saving expert embeddings to the database...\")\n",
    "expert_embeddings_df.to_sql('expert_embeddings', conn, if_exists='replace', index=False)\n",
    "print(\"Saving public embeddings to the database...\")\n",
    "public_embeddings_df.to_sql('public_embeddings', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "print(\"Embeddings calculated and saved to the SQLite database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize the scores and adjust the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Data Columns: Index(['id', 'text', 'author', 'created_utc', 'permalink', 'score',\n",
      "       'cleaned_text'],\n",
      "      dtype='object')\n",
      "Public Embeddings Columns: Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '375', '376', '377', '378', '379', '380', '381', '382', '383', 'id'],\n",
      "      dtype='object', length=385)\n",
      "Saving weighted public embeddings to the database...\n",
      "Weighted public embeddings calculated and saved to the SQLite database.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = \"/Users/vesper/Desktop/LSE/Capstone Project/dissertation/arctic_shift/filtered_data/relevant_data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load the cleaned public dataset and existing embeddings\n",
    "public_data = pd.read_sql_query(\"SELECT * FROM cleaned_public_data\", conn)\n",
    "public_embeddings_df = pd.read_sql_query(\"SELECT * FROM public_embeddings\", conn)\n",
    "\n",
    "# Ensure the correct columns are accessed\n",
    "print(\"Public Data Columns:\", public_data.columns)\n",
    "print(\"Public Embeddings Columns:\", public_embeddings_df.columns)\n",
    "\n",
    "# Normalize scores between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Assuming score normalization is only relevant for public data\n",
    "public_scores = public_data['score'].values.reshape(-1, 1)\n",
    "public_scores_normalized = scaler.fit_transform(public_scores).flatten()\n",
    "\n",
    "# Convert embeddings DataFrame to numpy array\n",
    "public_embeddings = public_embeddings_df.drop(columns=['id']).values\n",
    "\n",
    "# Adjust embeddings by normalized scores\n",
    "weighted_public_embeddings = public_embeddings * public_scores_normalized[:, np.newaxis]\n",
    "\n",
    "# Save the adjusted embeddings back to the database\n",
    "weighted_public_embeddings_df = pd.DataFrame(weighted_public_embeddings)\n",
    "\n",
    "# Adding 'id' column to match the original data for merging later\n",
    "weighted_public_embeddings_df['id'] = public_embeddings_df['id']\n",
    "\n",
    "# Save to a new table in the database\n",
    "print(\"Saving weighted public embeddings to the database...\")\n",
    "weighted_public_embeddings_df.to_sql('weighted_public_embeddings', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "print(\"Weighted public embeddings calculated and saved to the SQLite database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
